
\chapter{Conclusion and Outlook}
\label{conclusion}

\section{Conclusion}
In the course of this work, analyis methods for CTA-data have been tested.
As CTA is still in construction, we are limited on the use of monte carlo data.

The necessary steps to go from low level CTA-data to a high-level analysis have been performed
using the official analysis pipeline ctapipe, that is still actively being developed.
A high-level reconstruction based on the use of random forests has been worked out as alternative
to the existing algorithms.
Diffuse proton and gamma simulations have been used as training samples for the models, 
whereas the evaluation was done on pointlike gamma samples.

With this setup, gamma-/hadron separation works as expected and different algorithms 
for the source position reconstruction lead to roughly comparable angular
resolutions.
Due to the layout of the CTA (south) array, low energy events, on average, have lower multiplicities.
This makes is difficult to reconstruct the source position with the HillasReconstructor 
algorithm and a DISP-based approach can lead to improvements. This is especially true for
events of multiplicity 2 or 3 and can thus be relevant for the early stages of operation of CTA.

Applying a cut on the predicted gamma probability improves the reconstruction for both
algorithms confirming our gamma-/hadron-separation model.

In both cases the performance of the DISP-approach saturates at a few \si{\tera\electronvolt}
whereas the HillasReconstructor improves even beyond that.
This behaviour corresponds with the obtained R2-score for the DISP-estimation, indicating 
that the problem lies in the DISP-model itself rather than the stereoscopic
algorithm. NOCH IN DER ANALYSE NACH URSACHEN SUCHEN? TRAININGSSTATISTIK? LEAKAGE? SST GRÖẞE?

\section{Outlook}
A lot of potential to improve the analysis is still open:

The performed preprocessing uses a rather generic cleaning, with
parameters, that were generally accepted as being 
decent, but not overly optimised in the working group. Recently 
more relaxed parameters, especially for the LSTCam, have been used.
This is especially interesting as the low energy predictions 
seemed to be inferior to the high energy predictions.
Furthermore more complex cleaning methods could be inspected further.
Since the start of this thesis, two additional algorithms have been 
implemented into ctapipe:
An imcomplete three-threshold procedure used in CTA-MARS, which does not factor 
in timing information yet, and a procedure used in the FACT experiment.
The last one was added to ctapipe by us, but due to the high effort to
optimize the cleaning parameters we stayed with the default 
two-threshold tailcuts method.
In the future we will have access to a newly produced, official 
benchmark dataset, which will enable us to compare results between 
different analyses and working groups.
This will make it easier to optimize parameters of the analysis.

Future steps could also include an analysis of observed monoscopic 
LST data. Since the LST is currently the only telescope 
at LaPalma, an analysis can not rely on stereoscopic information.
The methods we employed for the gamma-/hadron-separation and 
DISP-calculation work in the stereocopic case, although further
optimisation will be needed.
A similar energy regression model has also been trained during the course
of the analysis. For both the monoscopic and stereoscopic case with
two stacked random forests, we were able to reproduce earlier works.

At last all models could be optimized in terms of the used architecture and
hyper parameters. The used architecture and parameters were based on earlier analyses 
at the FACT experiment and although random forests are generally 
robust in terms of the used hyper parameters, some gains might be 
possible.
Bigger differences might also be gained by testing different models.
For example, extremely randomized trees or boosting might hold
potential to get even better models for all the tasks.