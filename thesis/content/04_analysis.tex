\chapter{analysis}\label{analysis}

The analysis consists of multiple steps, that will be explained in the following.
All parameters for the analysis as well as the features used for the
different random forests can be found in the appendix (link setzen optimalerweise).

\section{Simulated Data and Preprocessing}
% configuration in appendix! + algorithmen beim prep

\subsection{Monte Carlo Data (More Infos: number of events, \ldots)}
Our analysis is based entirely on simulated monte carlo (MC) data.
The particle shower simulation is done with the software CORSIKA, 
the simulation of the instrument response goes by the name of simtel.

Monte carlo data is simulated and shared across the CTA-collaboration, 
our analysis uses the data from the PROD3-simulation of 
the CTA-south array. These simulation include way more telescopes 
than the array will eventually have. This is done to compare
different layouts and telescope positions.
We will choose the relevant telescopes in a way, that
our array resembles the earlier mentioned south array (See section \ref{sec:cta}).

The CORSIKA-simulations start with particles of a given energy that hit the atmosphere,
propagate through the air and arrive at the telescopes locations.
The SIMTEL-simulations include the instrument response and trigger settings to
evaluate if an event is going to be recorded.

Our data contains everything the telescopes are going to measure
at a low level
plus additional MC information, that will not be present at real experiments.
This includes the simulated particle direction and energy and type.
This ground-truth or MC-truth allows us to gauge the efffectiveness of our 
algorithms by comparing the predictions with the MC-truth values.

The total number of simulated data after the low level analysis 
is shown in table \ref{tab:mc_infos}.

\begin{table}
    \begin{center}
        \begin{tabular}{l r r r}
            \hline
            & Pointlike Gamma & Diffuse Gamma & Protons \\
            %\hline
            MC runs & \num{1113} & \num{17382} &  \num{9273} \\ 
            %\hline
            Array events & \num{955317} & \num{3409290} & \num{4657529} \\
            %\hline
            Telescope events & \num{4554313} & \num{14383270} & \num{20106385} \\
            %\hline
            Accumulated file sizes & 0 & 0 & 0 \\
        \end{tabular}
    \end{center}
    \caption{Take the infos from the simtel files instead! List file sizes aswell}
    \label{tab:mc_infos}
\end{table}

At the lowest available level our "observed" data consists of uncalibrated waveforms 
in the camera pixels.
We will perform several analysis steps that would also be needed for real data.
The choice of parameters for the low level processing 
is based on values from earlier studies, such as \cite{kai_diss}.
These will be listed in the appendix(Link setzen!).

To adress a specific event, we will split the event information into
three layers, that will be saved as separate dataframes:
\begin{enumerate}
    \item{$run$: This contains non-event-specific information about the monte carlo run, e.g.
    spectral\_index, particle injection height, ...}
    \item{$array\_events$: Any shower that triggered the array at some point 
    is considered to be an array event.
    Array-level features consist of array-wide shared event information 
    (e.g. number of triggered telescopes, average intensity),
    reconstructed high-level features of the primary particle (energy, source position, ...) and the 
    event specific monte carlo information to compare these features against.}
    \item{$telescope\_events$: This specifies how a specific telescope has seen the shower and
    contains information about the telescope itself (e.g.focal length). Telescope-level features 
    describe the camera image and contain reconstructed features that were retrieved based on 
    the telescopes specific measurements.}
\end{enumerate}
In the following we will 
distinguish between "array-events" and "telescope-events" when describing the reconstruction methods
with available information as mentioned above.
The number of triggered telescopes will be referred to as the event multiplicity.

\subsection{Reconstruction on the Telescope Tevel}
\label{sec:tel_analysis}

% definition datenlevel
- pixel value vs arrival time wording?
- calibration
- cleaning
- hillas parameters 
- telescope level
- tabellen der features bei runs/arrays/telescopes

To be able to apply high-level analysis methods, the initial data 
needs to be reduced and processed substantially.

Processing of the simtel-files is done with the aforementioned ctapipe, starting with 
the calibration of the event. This performs an integration of the waveforms in 
each pixel of the camera of a triggered telescope. 
This reduces the waveforms to two values each: An integrated value which describes 
the amount of light captured during the pulse and a peak value indication the arrival 
time of the measured pulse. These are referred to as 
photo-electrons and pulse time in the following.
Inside ctapipe the initial data is referred to as R1
and the resulting data as dl1. 

The resulting images get cleaned with a tailcuts approach before they are
used to calculate image features, such as the hillas parameters.
The tailcuts cleaning is a two-step method based on the pixel value:
First, all pixels with a value below an upper threshold get discarded from 
the image. Second, all discarded pixels above a lower threshold, that also 
happen to have at least a fixed amount of neighbouring pixels,
that survived the first step, get added back in.
Another cleaning method, that also includes the arrival time information, 
was implemented tested, but did not lead to any better results.
To keep the analysis more comparable to other works, we stayed with 
the tailcuts cleaning algorithm, which was the default algorithm at the time 
of this analysis.

After the image is cleaned, image features get calculated.
This includes the classic hillas parameters among others.
In total we calculate the following parameters:

\textbf{Hillas Parameters:}

These provide a general, basic description of the image and form the base for 
all subsequent analyses.
\begin{itemize}
    \item{$x, y$: Coordinates of the image cog}
    \item{$r, \phi$: Coordinates of the cog in polar coordinates}
    \item{$intensity$: Summed up pixel values}
    \item{$length, width$: Size of the image ellipse}
    \item{$\psi$: Angle between ellipse orientation and x-axis}
    \item{$skewness, kurtosis$: Higher order image moments along the shower axis}
\end{itemize}

\textbf{Leakage:}

These describe how much of the shower was captured by the camera.
A lot of information at the edge of the camera indicates that part of the light missed the telescope, 
which makes the hillas parameters less reliable.
Besides, this information is important to estimate the primary energy.

\begin{itemize}
    \item{$leakage\_1\_intensity$: Percentage of photo-electrons after cleaning in the outer most ring of pixels with respect to the photo-electrons in the whole image}
    \item{$leakage\_1\_pixel$: Percentage of pixels after cleaning in the outer most ring of pixels with respect to the pixels in the whole image}
    \item{$leakage\_2\_intensity$: Percentage of photo-electrons after cleaning in the two outer most rings of pixels with respect to the photo-electrons in the whole image}
    \item{$leakage\_2\_pixel$: Percentage of pixels after cleaning in the two outer most rings of pixels with respect to the pixels in the whole image}
\end{itemize}

\textbf{Concentration:}

These describe how the light is distributed inside of the image.
\begin{itemize}
    \item{$concentration\_cog$: Percentage of photo-electrons in the three pixels closest to the cog with respect to the photo-electrons in the whole image}
    \item{$concentration\_core$: Percentage of photo-electrons inside the hillas ellipse with respect to the photo-electrons in the whole image}
    \item{$concentration\_pixel$: Percentage of photo-electrons in the brightest pixel with respect to the photo-electrons in the whole image}
\end{itemize}

\textbf{Timing information:}

These describe the temporal evolution of the captured light.
This can be useful to improve the reconstruction of the source position.
\begin{itemize}
    \item{$slope, slope\_err$: Slope and corresponding  error of a linear regression to the pixel-wise pulse times along the main shower axis.}
    \item{$intercept, intercep\_err$: Intercept and corresponding  error of a linear regression to the pixel-wise pulse times along the main shower axis.}
    \item{$deviation$: Root-mean-square deviation between the individual pulse times and the predicted pulse time at the cog.}
\end{itemize}

\textbf{Other:}

The way clusters form in the image can give hints towards the classification of the primary particle.
\begin{itemize}
    \item{$num\_islands$: Number of individual clusters in the cleaned image.}
    \item{$num\_pixel\_in\_shower$: Number of pixels that survived the cleaning.}
\end{itemize}

All these features get used in the machine learning algorithms at later stages.

\subsection{Hillas Reconstructor}  % we dont seperate between 0 and 1 right?
After the image processing of each associated telescope image has been finished,
the predictions of our "baseline" source position estimator get calculated.
This algorithm is referred to as HillasReconstructor inside ctapipe, because 
it works based on the hillas parametrisations of the images.

For each triggered telescope, a 2D-plane is drawn into the spatial 3D-space based on the main shower 
axis and the telescope orientation. These planes intersect and 
the weighted average of all intersections gives the 
interaction point leading to the observed shower.
For a general illustration see figure \ref{fig:hillas_reconstructor}.

Intersecting the main shower axes on the ground frame leads to 
an estimation of the core position (the position where the 
shower hits the ground).

Together this can be used to reconstruct the shower origin.
Right now this is the default reconstruction algorithm inside ctapipe.
The current implementation does not provide an 
estimation for the uncertainty of the reconstructed parameters.

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{images/hillas_reco.png}
	\caption{Illustration of a stereoscopic reconstruction of the source position.
    From each telescope a plane is drawn, the direct intersection 
    on the ground leads to the impact point ($core$, red circle).
    The image is taken from a habilitation thesis by 
    Mathieu de Naurois \cite{hillas_reco}.
    Not included is the reconstruction of the main interaction point ($h_max$),
    that could be obtained by calculating a weighted average of the plane
    intersections.}
	\label{fig:hillas_reconstructor}
\end{figure}

Since this method inherently requires a stereoscopic experiment
and multiple triggered telescopes, it will not work for a single telescope.
From geometric considerations, it is expected
that this method works best with an event multiplicity 
$\geq 3$ with higher multiplicity leading to better results.

In total, this step leads to the following features on array level:
\begin{itemize}
    \item{$alt, az$: Reconstructed source position}
    \item{$core\_x, core\_y$: Reconstructed core position on the ground}
    \item{$average\_intensity$: Mean intensity over all triggered telescopes}
    \item{$h\_max$: Reconstructed shower interaction height}
\end{itemize}

\section{High-Level Analysis}

\begin{table}
    \caption{Number of remaining events for each simulated particle type after the
    preprocessing. These are the numbers that get split into different datasets 
    and used for the machine learning tasks.}
    \begin{center}
        \begin{tabular}{l r r r}
            %\hline
            & Pointlike Gamma & Diffuse Gamma & Protons \\
            \hline
            MC runs & \num{1113} & \num{17382} &  \num{9273} \\ 
            %\hline
            Array events & \num{955317} & \num{3409290} & \num{4657529} \\
            %\hline
            Telescope events & \num{4554313} & \num{14383270} & \num{20106385} \\
            %\hline
        \end{tabular}
    \end{center}
    \label{tab:events_after_prep}
\end{table}

High level analysis of the preprocessed data is based on the use of
the aict-tools \cite{aict-tools} package which itself makes use of
sklearn \cite{sklearn_api} for the implementation of the machine learning algorithms.
The aict-tools have originally been developed for the FACT-experiment
which is a single IACT. 
For this reason
adaptions had to be made compared to the status of the project at the start 
of this thesis to perform a stereoscopic analysis.

The algorithm of choice is the Random Forest algorithm
as it is well suited for the use with tabular data.
Model feature importance will be calculated
based on the sklearn functionality, which
uses the mean decrease impurity (See \cite{hastie2017springer} for a discussion on
feature importance estimations.).
Separate models are trained for the tasks of gamma/hadron
separation and signal source position
reconstruction.

To achieve more robust models and avoid overfitting, the data gets split into mutiple dataframes.
The gamma-/hadron-separation model and the DISP-/SIGN-models each get 
40\% of the available diffuse gamma data for training. 20\% are left out
as a test dataframe, that is completely unseen by the models during training.

The pointlike gamma events are not used for the training of the models,
but only to evaluate the models later.
For this all available data gets used.

We only use 30\% of the proton events to have a roughly even class distribution while training 
the gamma-/hadron-separation model (see table \ref{tab:events_after_prep}).



\subsection{Gamma-/Hadron-Separation}
For the task of gamma/hadron separation a random forest can be trained
using either only monoscopic information or also using array-level
information from earlier reconstruction steps.
Performance of the Classifier will be gauged based 
on the area under the receiver-operating-charateristic curve (AUC).
Using stereoscopic information
generally seems to improve the AUC by a few percent points.

The single telescope predictions can be combined by
simple functions such as the mean or median of the
single predictions to provide a prediction for the complete
array-event.

% features und kram


% \section{energy estimation}
% Energy estimation can be performed in the same way as the gamma/hadron
% separation. For this task there has been earlier work indicating
% the usefulness of a second machine learning model trained
% on the predictions of the first telesope-level model
% \cite{ba-lars}.

% I am thus going to present results based on either calculating the mean
% of the telescope level predictions and using a second random forest
% to improve the array level prediction.

\subsection{Reconstruction of the Source Position}
\label{sec:source_position}

The position on the shower axis can be estimated based on 
the hillas parameters and other image features, as 
explained in section \ref{sec:measuring}.

The calculation of the DISP is based on all the image-level
features listed in \ref{sec:tel_analysis}.
In this case this is done using machine learning models.
At this point the reconstructed source position
is fixed at two points at the main shower axis.

To resolve the head/tail ambiguity in monoskopic mode,
we will train a second random forest.
This is called SIGN-prediction, interpreting the two possible sides
as $\pm1$, allowing for binary classification.
From FACT-analyses we know that accuracies of 70-80\% should be achievable
if we make no mistakes \cite{some fact paper}.

For the stereoskopic analysis, we employ an approach inspired by 
what the MAGIC-collaboration does in some of their analyses.
In the case of the MAGIC-telescopes the ambiguity does not
get resolved until the individual results get combined
on the stereo level. The choice of the correct
pair out of the four reconstructed positions can be done either
by calculating the crossing point of both main shower axises
or by calculating the pairwise distances between the positions \cite{ALEKSIC201676}.

The method, where all four distances get calculated, is illustrated in figure \ref{fig:disp_magic}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/magic_stereo_disp.png}
    \caption{Illustration of the stereoscopic DISP-reconstruction used in magic.
        Two shower ellipses are drawn with the respective main shower axes as dashed lines
        and the two disp predictions each marked as 1A, 1B, 2A, 2B.
        The pairwise distances are displayed by the dotted lines.
        The closest pair is 1B-2B, the resulting prediction lies in the middle of
        the 2 points. The true source position is marked with a diamond.
        The illustration has been taken from a study of the Crab Nebula with MAGIC,
        showing the effect of extensive upgrades in 2011-2012 \cite{ALEKSIC201676}.}
    \label{fig:disp_magic}
\end{figure}

Since we have a variable number of telescopes per (array) event,
we need to adapt this method in a scalable way.

The most obvious approach seemed to be an iterative one:
For each pair of triggered telescopes, we combine the results 
in the way described above (In fact we forgo the part where events get discarded 
if the minimum distance is too big. This is done to keep the same event count as 
the HillasReconstructor to not accidentially create a bias in the dataset.)
From our research it seemed to have no positive effect to weight
the average of the two points based on common image features such as 
the intensity or size.

We then cache all intermediate results and average them to get the final prediction.
In this case taking the median of the pairwise predictions seemed to be more robust
than the mean, weighted or unweighted.

Figure \ref{fig:stereo_disp} illustrates this for the case of 4 triggered telescopes.


\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_all.pdf} 
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_2.pdf} 
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_1.pdf}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_3.pdf}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_4.pdf} 
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_6.pdf} 
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_5.pdf}
        \includegraphics[width=0.8\linewidth]{Plots/stereo_magic_result.pdf}
    \end{subfigure}
    \caption{(Plots anpassen, sodass das etwa wie beim magic ding aussieht. GGf auch Größe und Layout)
    Illustration of our iterative extension to the stereoscopic magic approach.
    Four telescopes have triggered, leading to a total of eight individual DISP-predictions (top left).
    For each pair of telescopes, the four predictoins get evaluated according to \cite{ALEKSIC201676}
    without dismissing poor events based on the minimum distance.
    This leads to 6 intermediate results, from which the median in
    alt and az respectively grant the final prediction.}
    \label{fig:stereo_disp}
\end{figure}

For this method we do not need the information from the SIGN model at all.
We will still train it and look at the results of the monoscopic
predictions to have a baseline for the LST-only analysis at the end of this thesis.


\section{Analysis for a single LST}
Inspired by the current status of CTA with only one LST operating and no MSTs or SSTs
on the north side just yet,
we wanted to add a completely monoskopic analysis.

This is in fact not a real monoscopic analysis and not performed on the north 
side as well. It is merely meant to show that the DISp+SIGn approach works as
expected for the monoscopic LST case.

(MEHR TELESKOPE?? VOLLES MODELL? ALLGEMEIN ALLES NOCH ZIEMLICH WHACKY HIER)
For this reason we chose a single telescope of the simulated data.
Choosing multiple telescopes and treating them as independent events 
would most likely be a valid option as well, although the 
events would not be 100\% uncorellated.

We perform the same analysis for this reduced dataset, leaving out only 
the stereoscopic parts of the analysis.
The machine learning models are expected 
to perform slightly worse as the stereoscopic features are missing.

As explained the HillasReconstructor does not work on a single telescope.
The only source-position predictions we can make and compare are monoscopic 
DISP+SIGN-predictions with the MC truth.